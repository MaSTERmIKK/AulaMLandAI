Approfondimenti
Funzioni di Attivazione:

ReLU (Rectified Linear Unit): f(x) = max(0, x). Introduce non linearità e aiuta a mitigare il problema del gradiente che svanisce.
Softmax: Converte un vettore di valori in probabilità, utile per la classificazione multiclasse.
Ottimizzatori:

Adam: Combina i vantaggi di altri ottimizzatori come AdaGrad e RMSProp. Adatta il tasso di apprendimento per ogni parametro.
Funzioni di Perdita:

Categorical Crossentropy: Misura la differenza tra due distribuzioni di probabilità, utile per problemi di classificazione multiclasse.
Metriche di Valutazione:

Accuracy: Percentuale di predizioni corrette.
Precisione, Richiamo, F1-score: Forniscono una valutazione più dettagliata delle prestazioni del modello, specialmente in presenza di classi sbilanciate.
